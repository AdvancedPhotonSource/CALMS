{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using 4 GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#Need only 1 GPU if loading 8-bit model\n",
    "print(device)\n",
    "\n",
    "print(\"Using %d GPUs\" %torch.cuda.device_count())\n",
    "\n",
    "import gradio as gr\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"eachadea/vicuna-13b-1.1\"\n",
    "tokenizer_path = \"./tokenizer/\"\n",
    "\n",
    "#Create a local tokenizer copy the first time\n",
    "if os.path.isdir(tokenizer_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "    os.mkdir(tokenizer_path)\n",
    "    tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8bfd79d5c4440c95e1d637bfb1dbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")#, load_in_8bit=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=2048,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0827000b534eb7a34bcd63c52fdb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# specify the path of the folder containing the files\n",
    "folder_path = \"APS-Science-Highlight\"\n",
    "\n",
    "# specify the string to search for in the files\n",
    "#search_string = \"Jonathan Lang\"\n",
    "\n",
    "# get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# initialize an empty string to store the file contents\n",
    "book = \"\"\n",
    "\n",
    "# iterate over the files in the folder\n",
    "for filename in tqdm(file_list):\n",
    "    # construct the full file path by joining the folder path and the filename\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    # check if the file is a file (not a folder)\n",
    "    if os.path.isfile(file_path):\n",
    "        # read the contents of the file and append to the file_contents string\n",
    "  #      with open(file_path, 'r') as f:\n",
    " #           if search_string in f.read():\n",
    "                with open(file_path, 'r') as f:\n",
    "                    book += \"\\n\\n\" + f.read()\n",
    "\n",
    "# print the contents of all files in the folder\n",
    "#print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embedding model and use that to embed text from source\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "texts = text_splitter.split_text(book)\n",
    "docsearch = FAISS.from_texts(\n",
    "    texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While some coronaviruses cause mild respiratory illness, highly pathogenic betacoronaviruses like those that cause SARS, MERS, and COVID-19 require effective therapeutics to reduce the severe, sometimes fatal, diseases they cause. To develop therapies for these diseases, it is critical to understand the structure and function of the viruses’ spike proteins. Spike (S) glycoproteins, which decorate the surface of coronaviruses, latch onto host-cell receptor proteins and undergo a dramatic structural change (from a “prefusion” to “postfusion” configuration) enabling the virus to forcefully fuse with the cell membrane. When designing therapies for coronaviruses, it’s more ideal to use antibodies that recognize prefusion spike proteins, since these antibodies create a stronger immune response. Other factors to consider are stability, mode of delivery, and production yield. For these reasons, the researchers opted to build on previous research which used camelid antibodies to study SARS and MERS.\n",
      "Llamas produce VHHs, also called Nanobodies, which are extraordinarily small and stable, with comparable antigen reactivities to conventional antibodies. To induce these antibodies, the researchers injected llamas with prefusion-stabilized MERS-CoV and SARS-CoV-1 S proteins. The S-directed VHHs were then isolated and in vitro neutralization assays were conducted to evaluate antiviral activity. The researchers discovered that two VHHs bind to the spike receptor-binding domains with high affinity, neutralizing SARS-CoV-1 and MERS-CoV.\n",
      "Since SARS-CoV-1 S and SARS-CoV-2 S are structurally similar, the researchers tested the VHHs for cross-reactivity against the SARS-CoV-2 RBD to find out if any of the VHHs could also prevent SARS-CoV-2 from infecting cells. While the SARS-CoV-1 RBD-directed VHH was shown to cross-react with the SARS-CoV-2 RBD and occlude the receptor-binding interface, the off-rate constant of the monovalent SARS VHH-72 was relatively high. The researchers, however, were able to overcome this issue by engineering a bivalent VHH-72-Fc, shown in Fig. 1, capable of neutralizing SARS-CoV-2 S pseudoviruses in vitro.\n"
     ]
    }
   ],
   "source": [
    "loc1 = 5\n",
    "print(texts[loc1-1])\n",
    "print(texts[loc1])\n",
    "print(texts[loc1+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Jonathan Lang?\"\n",
    "N_hits = 3\n",
    "docs = docsearch.similarity_search_with_score(query, k=N_hits)\n",
    "#for doc in docs:\n",
    "#    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author affiliations: 1McGill University, 2Sorbonne Université\n",
      "\n",
      "Correspondence: *martin.schmeing@mcgill.ca\n",
      "Correspondence: *awsch@uchicago.edu\n",
      "Published Date\n",
      "\n",
      "11.17.2020\n",
      "\n",
      "Studying Our Galaxy’s “Water Worlds”\n",
      "\n",
      "The original Arizona State University news release by Karin Valentine can be read here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get context strings\n",
    "context=\"\"\n",
    "for i in range(N_hits):\n",
    "    context += docs[i][0].page_content +\"\\n\"\n",
    "print (context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"context\"], template=template\n",
    ")\n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"history\", \n",
    "                                        input_key = \"input\", \n",
    "                                        k=6)\n",
    "\n",
    "\n",
    "conversation = LLMChain(\n",
    "        prompt=PROMPT,\n",
    "        llm=local_llm, \n",
    "        verbose=True, \n",
    "        memory=memory\n",
    ")\n",
    "\n",
    "#print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Context:\n",
      "Argonne National Laboratory seeks solutions to pressing national problems in science and technology. The nation's first national laboratory, Argonne conducts leading-edge basic and applied scientific research in virtually every scientific discipline. Argonne researchers work closely with researchers from hundreds of companies, universities, and federal, state and municipal agencies to help them solve their specific problems, advance America's scientific leadership and prepare the nation for a better future. With employees from more than 60 nations, Argonne is managed by UChicago Argonne, LLC for the U.S. Department of Energy's Office of Science.\n",
      "\n",
      "The U.S. Department of Energy's Office of Science is the single largest supporter of basic research in the physical sciences in the United States and is working to address some of the most pressing challenges of our time. For more information, visit the Office of Science website.\n",
      "\n",
      "Published Date\n",
      "\n",
      "04.09.2018\n",
      "The new undulator began operating at the Sector 7-ID x-ray beamline of the APS on January 19, 2018. This beamline, which is managed by the Argonne X-ray Science Division, is dedicated to ultra-fast time-resolved measurements of materials. Scientists are planning to use the new device for study the dynamics of fuel injection; a better understanding of that process could lead to more fuel-efficient motor vehicles.\n",
      "\n",
      "“They can bring this beam directly from the undulator and do a standard imaging experiment, like you would have an x-ray taken in a doctor’s office or a dental facility. But you can do it very fast here,” at intervals of a billionth of a second, said Jonathan Lang, director of the X-ray Science Division at the APS.\n",
      "Efim Gluskin, an Argonne Distinguished Fellow and former APS Division Director who has led the undulator program at the APS from its inception, likened the motion of the electron bunch in the HSCU to the looping motion of a coiling roller coaster like the X Flight ride at the Six Flags Great America amusement park near Chicago. As electrons corkscrew through the device’s magnetic field, they generate the circularly polarized radiation.\n",
      "\n",
      "But in order to force the spiral motions of electrons, a special magnet had to be built with a strong spiraling magnetic field. That goal was accomplished by wrapping superconducting wires around a corkscrew-shaped section of iron. The end result is a 1.1-meter-long superconducting electromagnet with many spiral-shape alternating north-south magnetic poles; these magnetic poles, when the HSCU is energized, are what send the electrons on their spiral path.\n",
      "\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Who is Jonathan Lang?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Jonathan Lang is the director of the X-ray Science Division at the Advanced Photon Source (APS) at Argonne National Laboratory. He oversees operations of several beamlines at the APS, including the Sector 7-ID beamline, which is used for ultrafast time-resolved studies of materials. Dr. Lang received his PhD in physics from Cornell University and conducted postdoctoral research at the Stanford Synchrotron Radiation Lightsource before joining Argonne in 2003.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HugFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
